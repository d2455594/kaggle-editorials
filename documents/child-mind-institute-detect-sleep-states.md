---
tags:
  - Kaggle
startdate: 2023-09-06
enddate: 2023-12-06
---
# Child Mind Institute - Detect Sleep States
https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states

**概要 (Overview)**

* **目的:** このコンペティションの目的は、子供たちが手首に装着した**加速度センサー（Accelerometer）**から収集された時系列データを用いて、**睡眠の開始（Onset）と覚醒（Wakeup）の正確な時刻を自動検出する**モデルを開発することです。
* **背景:** 睡眠は子供の心身の健康、学習、発達にとって極めて重要です。睡眠ポリグラフ検査（PSG）のような従来の睡眠検査は、環境や費用の制約がありますが、手首装着型センサー（アクチグラフィ）のようなウェアラブル技術を用いれば、自然な環境下での長期間の睡眠パターンモニタリングが可能になります。これにより、睡眠障害の研究、発達過程の追跡、介入効果の評価などが容易になります。Child Mind Instituteは子供の精神保健と学習障害に焦点を当てており、睡眠はその研究分野において重要な要素です。
* **課題:** 加速度データのみから睡眠状態、特に覚醒から睡眠への移行（Onset）や睡眠から覚醒への移行（Wakeup）を正確に検出することは困難です。静かに横になっている覚醒状態と睡眠状態の区別、あるいは寝返りが多い睡眠と活動的な覚醒状態の区別は、動きのデータだけでは曖昧な場合があります。また、子供の活動レベルや睡眠パターンには大きな個人差があり、日によっても変動します。センサーのノイズや装着状況による影響も考慮する必要があります。このタスクは、時系列データにおける**イベント検出 (Event Detection)** 問題と捉えられます。

**データセットの形式 (Dataset Format)**

提供される主なデータは、長期間にわたる加速度センサーの時系列データと、専門家によってアノテーション（注釈付け）された睡眠の開始・覚醒イベントの時刻情報です。

1.  **トレーニングデータ:**
    * `train_series.parquet`: 加速度センサーの時系列データ。各行が特定の記録 (`series_id`) 内のある時点 (`step` または `timestamp`) のデータに対応します。`series_id`, `step`, `timestamp`, 加速度データ（例: `anglez`, `enmo` - 動きの大きさを示す指標）などの列を含みます。
    * `train_events.csv`: **ターゲット変数**となる睡眠イベントの正解ラベル。各行が特定の `series_id` 内の一つの睡眠期間 (`night`) におけるイベントに対応します。`series_id`, `night`, `event` ('onset' または 'wakeup' のいずれか)、そしてそのイベントが発生した正確な `step` または `timestamp` の列を含みます。

2.  **テストデータ:**
    * `test_series.parquet`: テスト用の加速度センサー時系列データ。`train_series.parquet` と同様の形式です。
    * テストデータには正解のイベント情報は含まれません。

3.  **`sample_submission.csv`**:
    * 提出フォーマットのサンプル。通常、`row_id`（予測イベントの一意なID）、`series_id`、`step`（予測されたイベント発生ステップ）、`event`（予測されたイベントタイプ 'onset' または 'wakeup'）、`score`（予測の確信度、0から1）の列を持ちます。参加者は、テストデータ内の各 `series_id` に対して、検出した全ての 'onset' および 'wakeup' イベントの `step` と確信度 `score` を提出します。

**評価指標 (Evaluation Metric)**

* **指標:** **イベント検出のための平均適合率 (Average Precision - AP)** （許容誤差（トレランス）ウィンドウを用いた評価）
* **計算方法:**
    1.  モデルは、予測したイベント（onset/wakeup）の発生時刻（`step`）と確信度（`score`）を提出します。
    2.  各正解イベントに対して、事前に定義された**許容時間窓（トレランスウィンドウ）**（例：正解時刻の前後30分）内に、同じ種類の予測イベント（onsetはonsetと、wakeupはwakeupと）が存在するかを確認します。
    3.  予測イベントと正解イベントは、確信度スコアが高い予測から順に、かつ許容時間窓の制約内で、1対1でマッチングされます。
    4.  マッチング結果（True Positives - TP）、マッチしなかった予測（False Positives - FP）、マッチしなかった正解イベント（False Negatives - FN）に基づいて、確信度の閾値を変化させながら適合率-再現率曲線（Precision-Recall curve）を描画します。
    5.  平均適合率（AP）は、この適合率-再現率曲線の下部の面積として計算されます。
    6.  最終的なスコアは、異なる許容時間窓におけるAPを平均するなどして算出された **mean AP (mAP)** となります（具体的な計算方法はコンペティション規定によります）。
* **意味:** この指標は、時系列データにおけるイベント検出タスクの評価に適しています。単にイベントを検出できたかだけでなく、その**タイミングの正確さ**（許容時間窓内に収まっているか）と、**検出の精度**（不要なイベントを検出せず、必要なイベントを見逃さないか）を、モデルが示す確信度スコアも考慮して総合的に評価します。スコアは**高い**ほど（最大1.0）、モデルが睡眠の開始と覚醒のイベントを正しいタイミングで、かつ高い信頼性をもって検出できていることを示し、性能が良いと評価されます。

要約すると、このコンペティションは、子供の加速度センサーデータから睡眠の開始・覚醒時刻を検出するイベント検出タスクです。データは時系列のセンサーデータとイベント時刻ラベルで構成され、性能は許容時間窓を用いたイベント検出の平均適合率（高いほど良い）によって評価されます。

---
**全体的な傾向**

上位解法では、手首装着型センサーデータ（加速度、角度）から睡眠状態（就寝: onset, 起床: wakeup）の変化点を検出するために、時系列データを扱えるニューラルネットワーク（RNN系のGRU/LSTM、CNN系のUNet/WaveNet、Transformer）が多く用いられました。多くの解法で、単純なエンドツーエンドではなく、複数ステージのアプローチが採用されており、1段階目で候補イベントを検出し、2段階目（LightGBMなどの機械学習モデルやルールベース）で候補を絞り込んだり、スコアを再計算したりする戦略が目立ちます。特徴量エンジニアリングも重要で、元データからの統計量（標準偏差、差分など）、時間に関する特徴（時刻、曜日、周期性）、ノイズやデータの繰り返しパターンの検出などが活用されました。ターゲット（正解ラベル）の設計にも工夫が見られ、イベント周辺にガウシアンや減衰カーネルを適用したり、複数の時間閾値でターゲットを作成したりする手法が用いられました。後処理はスコア向上に不可欠で、ピーク検出アルゴリズム、Non-Maximum Suppression (NMS) や Weighted Box Fusion (WBF) のような手法、イベント時刻の微調整（秒単位のシフト）などが広く使われました。モデルのアンサンブルも一般的なテクニックでした。

**各解法の詳細**

**1位**

- **アプローチ:** CNN(DownSample) -> Residual GRU -> CNN(UpSample) をベースとした単一モデルで候補を検出し、精巧な2段階後処理で最終的なイベントを選択する。
- **アーキテクチャ (1st Level):** CNN (DownSample) - Residual GRU - CNN (UpSample)。入力スケーリングにSE Moduleを使用。最終層に分単位の特徴量（埋め込み）を接続。
- **アーキテクチャ (2nd Level):** LightGBM, CatBoost, CNN-RNN, CNN, Transformer など多様なモデルを使用。
- **アルゴリズム:** 減衰ターゲット（エポックごとに減衰）、周期性フィルター（ルールベース）、AdamWオプティマイザ。後処理アルゴリズム（2nd Levelモデルによる分単位確率予測、許容範囲スコア計算、スコア更新と候補選択の反復）。
- **テクニック:**
    - **データ準備:** 日単位チャンク分割（オフセットあり）、エポックごとにチャンクの一部を使用、評価時は重複区間を平均化し端をトリミング。
    - **ターゲット:** GTイベントからの距離に応じて減衰するターゲットを作成し、エポック毎にさらに減衰。
    - **特徴量:** カテゴリカル（時、分、曜日、周期性フラグ）、数値（anglez, enmoおよびその統計量、anglez差分の中央値）。
    - **後処理 (2nd Level):** 1st Levelの予測を分単位で集約しピーク検出。ピーク周辺のデータを使い、分単位のイベント有無確率を予測するモデル群（LGBM, CatBoost, NN系）を学習。評価指標に基づきスコアを計算し、最もスコアの高い候補点を採用。採用した点の影響を除去しながらスコアを再計算し、次の候補点を選択、を繰り返す。計算高速化にJITコンパイル利用。

**2位**

- **アプローチ:** 3段階のアプローチ。1段階目で1D CNN(U-Net)によりイベント候補とその信頼度を検出し、2段階目でLGBMを用いて1日2イベントの制約や長期的な特徴を考慮して再スコアリング、3段階目で予測を時間シフトさせて候補を追加しLGBMでスコアリング。
- **アーキテクチャ:** 1st Stage: 1D CNN (U-Netライク)。 2nd/3rd Stage: LightGBM。
- **アルゴリズム:** ピーク検出、LGBM回帰（2nd: 日次精度曲線予測、3rd: スコア予測）、Weighted Box Fusion (WBF) ライクなアンサンブル。
- **テクニック:**
    - **前処理/特徴量:** 1分集約、角度/ENMO統計量、時間特徴（sin/cos）、周期性特徴、高周波成分特徴など。
    - **1st Stage:** イベント検出と睡眠/覚醒分類を1D CNNで実施。ピーク検出などで候補抽出。
    - **2nd Stage:** LGBMで1st Stage候補の信頼度を再スコアリング。特徴量には1st Stageの信頼度、候補間の時間差、日内順位、日ごとの統計量などを使用し、「1日2イベント」制約や長期的な変動を考慮。
    - **3rd Stage:** 2nd Stageの予測を時間シフトさせ（例: ±12ステップ）、新たな候補を生成。LGBMでこれらの候補をスコアリング。
    - **アンサンブル:** 複数シード・モデルの1st Stage予測を平均化後、2nd/3rd Stageを実行。最終的にWBFライクな手法でアンサンブル。

**3位**

- **アプローチ:** GRU、UNET、LGBMのアンサンブル。特徴量エンジニアリングとデータ拡張を活用。
- **アーキテクチャ:** GRU、UNET、LightGBM。
- **アルゴリズム:** CrossEntropy損失。ピーク検出。
- **テクニック:**
    - **前処理/特徴量:** 日単位シーケンス（30秒粒度）、anglez絶対値、anglez/enmo標準偏差、ノイズ検出フラグ（同時間同値繰り返し）、時間頻度エンコーディング（時分レベル）。
    - **データ拡張:** 系列全体を反転。
    - **ターゲット:** Onset/Wakeupそれぞれで出力。ターゲットを近傍ステップ（前2、後1）に拡張。
    - **学習:** 7特徴量のみ使用。GRUとUNETは同様に学習。LGBMは限定的な貢献。
    - **アンサンブル:** GRU (x8), UNET (x2), LGBM (x1) の加重平均。
    - **後処理:** ピークを検出し、最適化された距離に基づいてフィルタリング。

**4位**

- **アプローチ:** UNetに変形を加えたアーキテクチャ（Encoder -> Bottleneck -> Transformer -> GRU/LSTM -> Decoder）。パッチ化によりシーケンス長を削減。正規化ガウシアンターゲットとカスタム損失関数を使用。後処理にWeighted Box Fusion (WBF) を適用。
- **アーキテクチャ:** Modified UNET (Encoder - Bottleneck - Transformer - GRU/LSTM - Decoder)。
- **アルゴリズム:** Cubic Loss (|y_true - y_pred|^3)。Weighted Box Fusion (WBF)。
- **テクニック:**
    - **前処理/特徴量:** 日単位シーケンス（17280ステップ）、パディング。特徴量（元系列、時間特徴: 時・曜日、派生系列特徴: 差分, HDCZAなど）。
    - **モデル:** パッチ化（3～6ステップ）でシーケンス長を削減。UNet Encoderの各層は初回層と結合。TransformerとRNN (GRU/LSTM) を組み込み。
    - **ターゲット:** 正規化ガウシアン。
    - **後処理:** Weighted Box Fusion (WBF) を適用。畳み込み平滑化、適応的ウィンドウサイズ、対数/線形重み付け、重み付き平均によるスコア計算、近傍抑制。ハイパーパラメータは手動+自動で調整。
    - **アンサンブル:** GRUベースx2、LSTMベースx2の計4モデルの平均＋チームメイトのモデル。最終提出は両者の予測をWBFで結合。

**5位**

- **アプローチ:** 4段階アプローチ。ヒューリスティックルールで候補生成、LightGBMでステップ補正、LightGBMでスコアリング、後処理で最終化。
- **アーキテクチャ:** LightGBM。
- **アルゴリズム:** LightGBM回帰 (L1 Loss)、LightGBM分類 (CrossEntropy)。カスタム後処理ルール。
- **テクニック:**
    - **候補生成:** ヒューリスティックルール（低活動領域、偽データ領域、周期的シフト）で候補ステップを生成。
    - **ステップ補正:** LightGBM回帰で候補ステップ位置を補正（ターゲット: `nearest_target_step - step`）。遠いターゲットの重みをゼロ化、近いターゲットほど重み付け。
    - **スコアリング:** LightGBM分類で各候補にスコアを付与（ターゲット: `max(0, 1 - |nearest_target_step - corrected_step| / 360)`）。ネガティブサンプルの重みを調整。Onset/Wakeupで別モデル。
    - **特徴量:** ウィンドウベース統計量（多様な集計・サイズ・操作）、時間特徴（minute%15など）、日単位集計特徴、候補間ギャップ、近傍候補との特徴量差など多数（2040次元）。
    - **データクリーニング:** 偽データ領域検出、手動での不良系列除去、低品質ターゲットの重みゼロ化。
    - **後処理:** ステップ調整（%12==0の場合+/-1）、近接候補（<720ステップ）のスコアに基づくフィルタリング、日単位スコア正規化（合計>1の場合）。

**6位**

- **アプローチ:** BiLSTMベースのUNetアーキテクチャ。分単位集計特徴量と重複パターン検出特徴量を使用。複数時間閾値でのターゲット予測とカスタム後処理。
- **アーキテクチャ:** BiLSTM-UNet (DownSample(LSTM+Pool) -> Transformer -> UpSample(LSTM+Upsample+SkipAdd))。
- **アルゴリズム:** 重み付きBCE損失。カスタム後処理（ピーク検出、近傍抑制、ステップ調整）。
- **テクニック:**
    - **データ準備:** 日単位分割（60分パディング、時間オフセット）、特定系列除去/トリミング。
    - **特徴量:** 分単位集計（anglez, enmo等の統計量）、minute%15、重複パターン検出フラグ（ハッシュ利用）。重複区間の元データはNull化。
    - **モデル:** UNet構造でLSTMを使用。スキップ接続は加算。Transformerブロックを中間層に配置。minute%15埋め込みを複数箇所で加算。Mish活性化関数。
    - **ターゲット/損失:** 複数の時間閾値（0, 1, 3, 5, 7分）内のイベント有無（計10出力/ステップ）。損失は閾値に応じて重み付け(`1/(threshold+1)`)したBCE。
    - **後処理:** 各ステップの予測値合計を計算。最高値を選択し、近傍（10分）を抑制、を繰り返す。ステップを分単位から元に戻す（x12）。ステップが12の倍数の場合、隣接予測値に基づき+/-1調整。
    - **学習:** AdamW、指数減衰LR、勾配クリッピング、時間セグメントDropOut。

**7位**

- **アプローチ:** WaveNetベースのモデル。複数日のコンテキストを入力。複数ヘッド出力とカスタム損失。オンラインハード例マイニング(OHEM)を利用。LightGBMによるスタッキング。
- **アーキテクチャ:** WaveNet (Wave_Block x3 + Dilated Conv)、Embedding (minute%15)、AvgPool/MaxPool、GroupNorm。出力ヘッドx2。
- **アルゴリズム:** CrossEntropy損失（実ターゲット用） + BCE損失（15分ウィンドウ用）。Online Hard Example Mining (OHEM)。カスタム後処理（ピーク検出、近傍抑制、低確率サンプリング）。LightGBMスタッキング。
- **テクニック:**
    - **データ準備/特徴量:** 3日間の分単位集計データ入力。特徴量（ターゲット、idx、anglez/enmo統計量、volatility(anglez差分中央値)、前日同分anglez差、時間）。
    - **モデル:** WaveNet構造。minute%15埋め込み。Group Normalization。AvgPool/MaxPoolによる集約特徴量追加。
    - **ターゲット/損失:** ターゲットを隣接分にも拡張。Near miss（ターゲット周辺2-3分）を損失計算から除外。2ヘッド出力（実ターゲット、15分ウィンドウ有無）。
    - **学習:** OHEM (50%)。6エポック、LR減衰。複数シード、一部データ除外やAugmentation有無で多様性確保。
    - **後処理:** スライディングウィンドウ予測。最大予測値を選択し、スコア（自身+近傍最大値）を記録。近傍ウィンドウ（+-4分）をゼロ化、周辺（+-20分）を0.5倍、を繰り返す。高スコア予測周辺の低確率予測もサンプリング。
    - **スタッキング:** LightGBMでNN予測値や派生特徴量を入力とし予測を改善（効果微小）。

**8位**

- **アプローチ:** エンドツーエンドを目指し、前処理/後処理を最小限に。2モデル構成。(1)Regressor(UNet)でイベント位置を回帰、(2)DensityNet(UNet+Transformer)でイベント確率密度を予測。
- **アーキテクチャ:** Regressor: 1D UNet + 1D ResNet backbone。 DensityNet: 1D UNet + Transformer Encoder (中間層)。
- **アルゴリズム:** Regressor: Huber Loss。 DensityNet: CrossEntropy Loss（Laplace分布平滑化ターゲット）。ALiBi位置エンコーディング。Gaussian Kernelによるスコア蓄積。ピーク検出。NMS。Matrix Profile。
- **テクニック:**
    - **Regressor:** ローカル情報（anglezのみ）を使用。ターゲットはイベントへの相対位置。損失はHuber Loss。推論時は各ステップの予測相対位置からGaussian Kernelでスコアを蓄積しピーク検出。NMS適用。
    - **DensityNet:** グローバル情報モデリングのためTransformer Encoder追加。2日間の区間を入力。ターゲットはイベント位置の確率密度（Laplace分布で平滑化）。損失はCrossEntropy + 区間内イベント有無予測。ALiBi使用。推論時は1日区間で予測。
    - **スコアリング:** Regressorが提案した位置候補に対し、DensityNetの予測確率密度から条件付き確率を計算しスコアとする。偽データ区間予測確率も考慮。
    - **後処理:** ステップ調整（xx:xx:15, xx:xx:45）。Augmentation（イベント位置摂動）。Matrix Profileで偽データ区間の予測を除去。
    - **アンサンブル:** Regressorは異なるwidthで学習した3モデルを平均。DensityNetは異なる入力（anglez, enmo, anglez+time）で学習したモデルを使用。

**9位**

- **アプローチ:** チームによる複数モデル（U-Net, LSTM-Transformer, GRUなど）と後処理手法の組み合わせ。詳細は各メンバーの記事参照。
- **アーキテクチャ:** 多様 (U-Net, LSTM-Transformer, GRUなど)。
- **アルゴリズム:** 多様。カスタム後処理（スコア補正、フィルタリング）。
- **テクニック:** メンバーごとに異なる特徴量エンジニアリング、モデル構築、学習戦略を採用。Youri氏による後処理が重要。

**10位**

- **アプローチ:** 1D-UNet + GRUモデル。特徴量エンジニアリングとカスタムターゲット設計。多数のモデルアンサンブル。
- **アーキテクチャ:** 1D-UNet + GRU。
- **アルゴリズム:** BCE損失。AdamWオプティマイザ。SWA (Stochastic Weight Averaging)。カスタム後処理（平滑化、ピーク検出、低スコア候補追加）。
- **テクニック:**
    - **特徴量:** enmo、anglez差分絶対値、時間エンコーディング(sin/cos)、重複データフラグ、測定開始からの時間フラグ。
    - **ターゲット:** 混合ガウシアンヒートマップ（広/狭シグマの加重和）。
    - **学習:** 12時間ランダムサンプリング、AdamW、SWA。
    - **アンサンブル:** 120モデルのアンサンブル。
    - **後処理:** 出力値を平滑化。閾値(0.01)以上の全ローカル最大値を候補に。残りのタイムスタンプを出力値降順に処理し、既存候補から十分（21分）離れていれば低スコア(0.1*出力値)で候補追加。


