---
tags:
  - Kaggle
startdate: 2023-08-30
enddate: 2023-11-18
---
# Google - Fast or Slow? Predict AI Model Runtime
https://www.kaggle.com/competitions/predict-ai-model-runtime

**概要 (Overview)**

* **目的:** このコンペティションの目的は、与えられたKaggleノートブック（AIモデルのトレーニングや推論コードを含む）が、特定のハードウェア環境（例：Kaggleの標準GPU環境）で**実行完了するまでにかかる時間（ランタイム）を予測する**モデルを開発することです。
* **背景:** AIモデルの実行時間を事前に知ることは、計算リソースの計画、コードの最適化、予算管理、ユーザーへの推定完了時間の提示などにおいて非常に重要です。特にKaggleのノートブック環境のように実行時間制限がある場合、ランタイムを予測できればタイムアウトを避け、より効率的に開発を進めることができます。
* **課題:** ノートブックには多種多様なコード（単純なデータ処理から複雑な深層学習モデルまで）、ライブラリ依存関係、ループ構造、データサイズなどが含まれており、静的なコード情報から動的な実行時間を正確に予測することは本質的に困難です。また、実行時間はハードウェア（CPU、GPUの種類、メモリ量）に大きく依存します。さらに、ネットワーク遅延やシステム負荷などの外部要因も実行時間に影響を与える可能性があります。ノートブックのコードから効果的な特徴量（例：セル数、演算の種類、ライブラリの使用状況）を抽出し、それらを用いてランタイムを予測するモデルを構築する必要があります。これは**回帰（Regression）** タスクです。

**データセットの形式 (Dataset Format)**

提供される主なデータは、Kaggleノートブックファイルとその実際の実行時間です。これは通常「Code」コンペティション形式で提供されます。

1.  **トレーニングデータ:**
    * `train.csv` (など): トレーニング用ノートブックのメタデータと実行時間。通常、`id`（ノートブック識別子）と**ターゲット変数**である `runtime`（秒単位の実行時間）の列を含みます。コードサイズやセル数などの事前抽出された特徴量を含む場合もあります。
    * `train_notebooks/` (など): 実際のノートブックファイル（`.ipynb`形式など）が格納されたディレクトリ。`id` に対応するノートブックが含まれます。参加者はこれらのノートブックファイルを解析して特徴量を抽出する必要があります。

2.  **テストデータ:**
    * `test.csv` (など): テスト用ノートブックのメタデータ（例: `id`）。
    * `test_notebooks/` (など): 実行時間を予測する必要があるテスト用のノートブックファイル。
    * テストデータには正解の `runtime` は含まれません。提出されたモデル（多くの場合、それ自体がKaggleノートブック）が、隠されたテストノートブックに対して実行され、予測値を生成します。

3.  **提出形式 (Submission Format):**
    * 通常、Kaggleノートブックとして提出します。
    * この提出ノートブックは、提供されるテストデータ（ノートブックファイルやメタデータ）を読み込み、各テストノートブックのランタイムを予測し、指定された形式の `submission.csv` ファイルを出力する必要があります。
    * `submission.csv` ファイルには、`id`（テストノートブック識別子）と `runtime`（予測された実行時間）の列が含まれます。

**評価指標 (Evaluation Metric)**

* **指標:** **スピアマンの順位相関係数 (Spearman's Rank Correlation Coefficient, Spearman's ρ)**
* **計算方法:**
    1.  実際の実行時間（真の値）を小さい順にランク付けします。
    2.  モデルが予測した実行時間を小さい順にランク付けします。
    3.  これら2つの**順位（ランク）** のリスト間のピアソンの積率相関係数を計算します。
* **意味:** この指標は、モデルが予測したランタイムの**順序**が、実際のランタイムの**順序**とどれだけよく一致しているかを評価します。絶対的な誤差の大きさ（例：RMSEで評価されるもの）よりも、どのノートブックが他のノートブックより速くまたは遅く実行されるかを正しく順位付けできているかを重視します。これは、スケジューリングや優先順位付けにおいて、正確な秒数を予測することよりも重要である場合が多いためです。また、外れ値の影響を受けにくいという特徴もあります。スコアは-1から1の範囲を取り、**高い**ほど（1に近いほど）、予測された順位と実際の順位の間の正の相関が強いことを示し、モデルの性能が良いと評価されます。

要約すると、このコンペティションは、Kaggleノートブックの実行時間を予測する回帰タスクです。データはノートブックファイルと実際の実行時間で構成され、性能は予測された実行時間の順序と実際の実行時間の順序の一致度を示すスピアマンの順位相関係数（高いほど良い）によって評価されます。

---
**全体的な傾向**

上位解法では、TPUの計算グラフを入力としてAIモデルの実行時間を予測するために、グラフニューラルネットワーク（GNN）を用いたアプローチが主流でした。特にSageConvが広く採用されましたが、GAT、GIN、TransformerベースのGNNも利用されています。大量のデータと複雑なグラフ構造を扱うため、グラフのプルーニング（削減）や圧縮、部分グラフの利用、効率的なデータローディングといったテクニックが重要でした。コンフィグレーション間の相対的な実行時間を学習するため、ペアワイズ損失（Pairwise Hinge Lossなど）やリストワイズ損失（ListMLEなど）といったランキング学習の手法が広く用いられました。特徴量エンジニアリング（ノード特徴の標準化/対数変換、埋め込み層の活用、追加特徴量の抽出）や、Attentionメカニズムの導入（Self-Channel Attention, Cross-Config Attention, Linformerなど）、モデルのアンサンブルもスコア向上に寄与しました。多くの解法で、データセット（layout, tile）やサブセット（xla, nlp, default, random）ごとにモデルを構築・学習する戦略が取られました。

**各解法の詳細**

**1位**

- **アプローチ:** グラフプルーニングとデータ圧縮で効率化し、GNN（SageConv + Attention）で学習。ランキング損失を使用。
- **アーキテクチャ:** Linear -> 2 x (InstanceNorm -> SAGEConv -> SelfChannelAttention -> CrossConfigAttention -> +residual -> GELU) -> Global Mean Pooling -> Linear。
- **アルゴリズム:** SAGEConv、Self-Channel Attention、Cross-Config Attention、PairwiseHingeLoss、AdamW。
- **テクニック:**
    - **データ準備:** グラフプルーニング（設定関連ノードとその1ホップ近傍のみ）、重複コンフィグ削除（layout）、特徴量圧縮（node_config_featをBase-7エンコード）、node_featのパディング値を-1に変更。
    - **前処理:** node_feat[:134]にStandardScaler、node_feat[134:]とnode_config_featに共有埋め込み層、node_opcodeに別埋め込み層。
    - **学習:** 提供されたtrain/val/test分割を使用。グラフごとに64または128コンフィグをサンプリング。
    - **Attention:** Self-Channel Attention（Squeeze-and-Excitation風）、Cross-Config Attention（コンフィグ間の比較）。
    - **推論:** Cross-Config Attentionを利用したTTA（複数回の並び替えと平均化）。
    - **アンサンブル:** 5-10モデルの単純平均。

**2位**

- **アプローチ:** SageConvベースのGNNをグラフ全体に適用。ランキング損失（ListMLE, DiffMat Loss）を使用。データ前処理と階層的バッチ構成が特徴。
- **アーキテクチャ:** SageConv（複数層、Residual接続あり）、Linear（特徴量変換、最終出力）。
- **アルゴリズム:** SageConv、ListMLE、DiffMat Loss（新規提案: 差分行列の上三角にMargin Ranking Lossを適用）、MAPE損失（XLAのみ併用）、Adam/AdamW、Step学習率スケジュール。
- **テクニック:**
    - **データ前処理:** 重複コンフィグ削除（最小ランタイム保持）、不良データ（unet_3d, mlperf_bert）削除、Layoutデータ再パッキング（個別ロード高速化）。
    - **特徴量:** ノードタイプ埋め込み、ノード特徴量圧縮（sign*log）、コンフィグ特徴量（Tileは全ノードにブロードキャスト）、早期融合（Early Fusion）。
    - **学習:** 提供された分割を使用。階層的バッチ構成（グラフ単位、コンフィグ単位のマイクロバッチ）。予測値のノルムクリッピング（ListMLE用）。

**3位**

- **アプローチ:** GNN（GPSレイヤー: SageConv + Linformer + RWPE）を使用。特徴量エンジニアリングとグラフ修正（プルーニング/マージング）を活用。
- **アーキテクチャ:** GPS Layer (SAGEConv + Linformer + RWPE)、SiLU活性化関数、Layer Normalization。
- **アルゴリズム:** SAGEConv、Linformer Attention、Learnable Positional Encoding (RWPE)、PairwiseHingeLoss、Adam、Cosine Annealing Scheduler。
- **テクニック:**
    - **特徴量:** 全140特徴＋Protobufから追加抽出（dynamic comフラグ、dot/convの引数形状、次元mod/div特徴など）。入力特徴量に対数変換適用。OPコード埋め込み。
    - **位置エンコーディング:** Random Walk Positional Encoding (RWPE)。
    - **グラフ修正:** 3種類のプルーニング/マージング戦略（①設定ノードのみ、②設定ノード+入出力、③②に加え非関連ノードをマージ）。仮想出力ノード追加。
    - **学習:** 複数コレクション同時学習後、個別ファインチューニング。異なるプルーニング戦略で学習したモデルを最終アンサンブルに使用。

**4位**

- **アプローチ:** グラフ構造情報を限定的に利用するシンプルなMLP。特定の特徴量インデックスを選択。
- **アーキテクチャ:** MLP (Embedding -> Sequential(Linear+ReLU) x2 -> MatMul -> Sequential(Linear+ReLU) x2)。
- **アルゴリズム:** 記載なし（損失関数等不明）。
- **テクニック:**
    - **特徴量選択:** node_featから12次元、node_config_featから8次元を選択。
    - **モデル:** Opcodeを埋め込み。ノード特徴とOpcode埋め込みを結合してMLP処理 (node_dense)。設定特徴と全ノード特徴（繰り返し）を結合してMLP処理 (config_dense)。両者の行列積と設定特徴の平均を結合して最終MLP (output)。

**5位**

- **アプローチ:** GraphSageベースのGNN。次元不変性を考慮したTransformerベースの特徴量埋め込み。双方向畳み込み。
- **アーキテクチャ:** GraphSage（3層、双方向畳み込み）、Transformer Encoder（次元特徴量埋め込み用）。
- **アルゴリズム:** GraphSage、PairwiseHingeLoss（グラフ内ペア＋バッチ内全ペア）、AdamW、Cosine Annealing Scheduler、DropEdge。
- **テクニック:**
    - **特徴量:** 次元特徴量（6次元x30特徴）をTransformerで埋め込み（トークン和で次元削減）。ユニークな特徴ベクトルのみ計算しコピーして効率化。Opcode埋め込み工夫（単項演算子を共通化）。設定可能ノードの特徴量をコンフィグ特徴量で上書き。入力特徴量に対数変換適用。
    - **学習:** グラフ全体を入力。オーバーサンプリング。Layoutデータのメモリマップモード読み込み。
    - **モデル:** Tileデータセットのみで学習したモデルと、Layout(+Tile)データセットで学習したモデル。
    - **アンサンブル:** Naiveモデル（特徴量Flatten）とTransformer埋め込みモデルのアンサンブル。

**6位**

- **アプローチ:** 設定関連ノードの5ホップ近傍の部分グラフを使用するGNN（SageConvまたはGATConv）。グラフインスタンス正規化とランキング損失を利用。
- **アーキテクチャ:** SageConv（4層、Residual接続）またはGATConv（4層）。
- **アルゴリズム:** SageConv、GATConv、Pairwise Ranking Loss（Layout）、ListMLE（Tile）、Graph Instance Normalization（ノード単位）。
- **テクニック:**
    - **データ準備:** 5ホップ近傍の部分グラフを使用。
    - **学習:** 勾配蓄積（サブグラフ単位）。
    - **正規化:** Graph Instance Normalization（ノード単位）。

**7位**

- **アプローチ:** GraphSAGE Transformer (GST) ベースのGNN。コンフィグサンプリングによるメモリ/時間削減。複数モデルのアンサンブル。
- **アーキテクチャ:** GSTベースのGNN。
- **アルゴリズム:** 記載なし（損失関数等不明）。
- **テクニック:**
    - **データ準備:** 各グラフから500または1000コンフィグをサンプリング。
    - **学習戦略:** 全体で学習するモデル、グラフの形状（エッジ数、ノード数）に基づき分割して学習するモデル、パラメータ（グラフ畳み込みタイプ、LR、バッチサイズ、層数、隠れ層サイズ）を変えたモデルを学習。
    - **アンサンブル:** 上記複数モデルのアンサンブル。

**8位**

- **アプローチ:** LightGBMを用いたシンプルな表形式データアプローチ。テストデータごとに最も類似した訓練データ（モデルタイプが同じと推測されるグラフ）を見つけて利用。
- **アーキテクチャ:** LightGBM。
- **アルゴリズム:** CrossEntropy損失（ターゲットをMinMaxScaler変換後）。
- **テクニック:**
    - **特徴量エンジニアリング:** Tile: 統計量（カウント、mean, max, std, last）。Layout: node_config_featをFlattenし、一意値列・重複列を削除。
    - **データ選択:** 各テストデータに対し、エッジ数・ノード数がほぼ同じ訓練データを特定し、そのデータのみで学習。
    - **学習:** ターゲットをMinMaxScalerで[0,1]に変換し、CrossEntropy損失で学習。固定ラウンド数で学習（検証データ不使用）。

**9位**

- **アプローチ:** GSTベースのGNN。グラフ圧縮（Dijkstraアルゴリズム利用）と特徴量の対数変換が特徴。データセット/モデルタイプ別の学習とアンサンブル。
- **アーキテクチャ:** GSTベース (SageConv)、TransformerConv、GATConvも試行。
- **アルゴリズム:** Dijkstraアルゴリズム（グラフ圧縮用）、PairwiseHingeLoss。
- **テクニック:**
    - **グラフ圧縮:** Dijkstraアルゴリズムを使用し、特定ノード(s)から設定関連ノード(node_config_ids)への最短経路上のノード・エッジのみを抽出してグラフを圧縮。
    - **前処理:** ノード特徴量に対数変換適用。
    - **学習:** 各グラフから512コンフィグをサンプリング。4つのデータタイプ（xla-random, xla-default, nlp-random, nlp-default）で別々にモデルを学習。さらに、グラフIDやノード数から推定されるモデルアーキテクチャ（ResNet, EfficientNet, BERTなど）ごとに特化したモデルも学習。
    - **アンサンブル:** 複数アーキテクチャ、複数データセットで学習したモデルをアンサンブル。

**10位**

- **アプローチ:** Graph Transformerベース。コンフィグ情報を中間層で融合（Intermediate Fusion）。Linear Attentionで計算量を削減。
- **アーキテクチャ:** Graph Transformer (Local Graph Block (APPNPなど) + Global Self-Attention (Linear Attention) + Cross-Attention)。
- **アルゴリズム:** Multi-head Linear Attention、APPNP (Graph Module)、ListMLE損失、AdamW、Cosine Annealing。
- **テクニック:**
    - **特徴量:** 入力特徴量に対数変換適用（log(x+3)）。
    - **コンフィグ融合:** 中間層でノード表現とコンフィグ情報を融合（Cross-Attention）。
    - **Attention:** 計算量削減のためLinear Attentionを使用。活性化関数はELU+1。
    - **モデル:** TransformerのMLP部分をグラフネットワーク（APPNPなど）に置き換え局所的混合を導入。Tile/Layout、XLA/NLPで別モデル構築。
    - **学習:** 各バッチで多数（～1000）のコンフィグを使用。Layoutデータは設定情報のみオンデマンドでロード。
    - **アンサンブル:** 多数（10-20）の異なる設定のモデルを混合。

