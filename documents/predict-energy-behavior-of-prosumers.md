---
tags:
  - Kaggle
  - 時系列データ分析
  - エネルギー
startdate: 2023-11-03
enddate: 2024-05-01
---
# Enefit - Predict Energy Behavior of Prosumers
https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers

**全体的な傾向:**

このコンペでは、プロシューマー（需要家であり発電者でもある）のエネルギー行動を予測することが課題です。上位解法では、時系列データの特徴を捉えるためのテクニックと、機械学習モデルのアンサンブルが広く用いられています。XGBoost、LightGBM、GRUといったモデルが頻繁に登場し、特徴量エンジニアリング（特にラグ特徴量）、検証戦略、オンライン学習などが重要な要素となっています。生産（production）と消費（consumption）を別々にモデル化するアプローチも一般的です。

**各解法の詳細:**

**1位**

- **アプローチ:** 生産量と消費量をターゲットとし、それぞれに対してXGBoostとGRUモデルを組み合わせたアンサンブル。オンライン学習戦略も採用。
- **アーキテクチャ:**
    - XGBoostモデル（4つ）：2つのターゲットと2つの `is_consumption` フラグの組み合わせ。
    - GRUモデル（2つ）：2つのターゲット。
- **アルゴリズム:** XGBoost、GRU（ゲート付き回帰型ユニット）。
- **テクニック:**
    - **検証戦略:** 最初の500日を学習データ、残りをホールドアウト検証セットとして使用。
    - **特徴量エンジニアリング:** 全てのテーブルを結合し、多数のラグ特徴量を作成。外部の議論からの特徴量（`fw_new_feature`）も利用。ターゲットに応じて異なる特徴量やターゲットを生成するアイデアにも言及。
    - **ターゲット:** `(target-target_shift2)/installed_capacity` と `target/installed_capacity` を主に使用。
    - **モデル:** XGBoostのハイパーパラメータはあまり調整せず。GRUモデルはシンプルな2層構造。カテゴリカル特徴量（county, product_type, hour, month, weekday, day）も利用。
    - **オンライン学習:** 公開LBでは効果は薄かったが、プライベートLBでは重要になると考え、毎月モデルを再学習する戦略を採用。
    - **試行されなかった手法:** 他のノートブックからの太陽光発電関連の特徴量、1D CNN、Transformerモデル、複数日の入力（GRU）、`is_business` の値によるモデル分割。

**5位**

- **アプローチ:** 生産と消費を別々にモデル化し、LightGBMのアンサンブルを使用。特徴量選択とエンジニアリングを重視。オンライン再学習も実施。
- **アーキテクチャ:** 生産用と消費用それぞれに、3つのLightGBM回帰推定量からなる`VotingRegressor`アンサンブル。
- **アルゴリズム:** LightGBM。
- **テクニック:**
    - **検証戦略:** 2段階クロスバリデーション。1段階目は時系列3分割CV（各フォールドで2ヶ月のテストデータ）、2段階目は残りの3ヶ月のデータで単純分割。最終モデル選択は公開テストデータに基づく。
    - **特徴量選択とエンジニアリング:** 多数の実験を実施。`MLflow` を活用。生産用には75個、消費用には85個の特徴量を使用。ラグ特徴量、過去1週間の天気予報ローカル平均との差分、ターゲットの変換（ラグ、比率、平均、差分）、三角関数を用いた時間関連特徴量、祝日関連特徴量、`ssrd_*` 特徴量など。ガス価格と国全体の過去の天気は除外。
    - **ターゲット変換:** 生産用には `target / (1 + installed_capacity)`、消費用には `target / (1 + target_avg)`。欠損値は類似ユニットの中央値で補完。
    - **モデル:** LightGBMのハイパーパラメータはカスタム進化的探索と2段階CVで微調整。
    - **オンライン再学習:** スコアリングデータの9日ごとにモデルを再学習（合計約10回）。
    - **試行されなかった手法:** `Optuna` を用いた特徴量選択（最終的にはカスタム進化的探索を使用）。

**6位**

- **アプローチ:** 生産と消費を別々にモデル化し、LightGBMのアンサンブルを使用。特徴量は公開ノートブックのものをベースに、単純なベースラインモデルの予測値を特徴量として追加。再学習はベースラインモデルのみ。
- **アーキテクチャ:** 生産用と消費用それぞれに、4つの異なるターゲットで学習したLightGBMモデル（合計8モデル）。それぞれ10シード、2つのハイパーパラメータ設定で学習（合計160モデル）。
- **アルゴリズム:** LightGBM。
- **テクニック:**
    - **特徴量エンジニアリング:** 公開ノートブックのものをベースに、ターゲットに合わせてわずかに変換。消費量と生産量それぞれの単純なベースラインモデルを作成し、毎日再学習して特徴量として追加。
    - **ターゲット:** `target - target_lag48`、`target/(target_lag48_avg + 1) - baseline_pred`、`target/(target_lag48_avg + 1) - target_lag48`、`target/(target_lag48_avg + 1)`。
    - **再学習:** LightGBMモデルは再学習せず。ベースラインモデルのみ毎日再学習。
    - **ブレンド:** 異なるターゲット、10シード、2つのハイパーパラメータ設定で学習したモデルを重み付きで組み合わせ。

**7位**

- **アプローチ:** 生産と消費を別々にモデル化。ターゲットとして、過去のターゲット値との差分を予測する自己回帰モデルの残差を使用。LightGBM、XGBoost、CatBoost、HistGradientBoostingRegressorをOptunaで最適化し、アンサンブル。
- **アーキテクチャ:** LightGBM、XGBoost、CatBoost、sklearnのHistGradientBoostingRegressor。
- **アルゴリズム:** LightGBM、XGBoost、CatBoost、HistGradientBoostingRegressor。
- **テクニック:**
    - **ターゲット:** 生産量には `normalized_production - normalized_production_lag48h`。消費量にはより複雑な自己回帰モデルの残差 (`normalized_consumption - 0.5 * normalized_consumption_lag48h - 0.1 * normalized_consumption_lag76h - ...`)。
    - **特徴量エンジニアリング:** 公開ノートブックと同様の特徴量（county, product_type, is_businessの組み合わせ、時間関連特徴量、eic_count, installed_capacity、価格、天気予報と過去の天気、祝日、過去のターゲット値とその統計量など。合計192個）。
    - **モデル:** LightGBM、XGBoost、CatBoost、HistGradientBoostingRegressorのハイパーパラメータをOptunaで最適化。5分割スライディングウィンドウCV。カスタム早期停止ルール。
    - **アンサンブル:** 上位3モデルの単純平均。異なるシードで学習したモデルを複製。最終アンサンブルは、LGBM（ベストパラメータ、2シード）、LGBM（セカンドベストパラメータ）、XGBoost（ベストパラメータ、2シード）。
    - **トレーニング:** 10モデル（生産5、消費5）を2月初旬に一度だけ学習。
    - **ポスト処理:** いくつかのアイデアを試したが、性能向上は見られず。