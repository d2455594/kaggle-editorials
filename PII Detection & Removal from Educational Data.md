---
tags:
  - Kaggle
url: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data
startdate: 2024-01-18
enddate: 2024-04-24
---
**全体的な傾向:**

上位解法は、Transformerベースの言語モデル（特にDeBERTa-v3 large）のファインチューニングが中心でした。外部データ（特にAI生成データ）の活用、カスタムヘッドの設計、データ拡張、そしてルールベースのポストプロセッシングが重要なテクニックとして用いられました。また、トークナイザの特性を理解し、それに対応した処理を行うこともスコア向上に貢献しました。

**各解法の詳細:**

**1位**

- **アプローチ:** 多様で大規模なAI生成データセットを活用したDeBERTaアンサンブル。カスタムモデル（Multi-Sample Dropout、Bilstmレイヤー）、知識蒸留、データ拡張、そしてルールベースのポストプロセッシング。
- **アーキテクチャ:** DeBERTa-v3 large/base、カスタムヘッド（Multi-Sample Dropout、Bilstmレイヤー）。
- **アルゴリズム:** ロス関数（CrossEntropyLoss、KLDivLoss）、オプティマイザ（AdamW）。
- **テクニック:**
    - **データ:** 外部データセット（nbroad、mpware、自作）、データ拡張（名前の入れ替え）。
    - **モデリング:** DeBERTaのバリエーション、知識蒸留（複数の教師モデル）、重み付き投票アンサンブル。
    - **ポストプロセッシング:** 閾値調整、`NAME_STUDENT` のフィルタリング（title-cased、数字/アンダースコアなし）、同一文書内でのラベル伝播、`PHONE_NUM` を `ID_NUM` に変換、`STREET_ADDRESS` の修正、`USERNAME` の修正、短すぎる/長すぎる予測の除去、`URL_PERSONAL`、`EMAIL` のフィルタリング、正規表現。

**2位**

- **アプローチ:** DeBERTa-v3-largeモデルのアンサンブルと、pre/post処理。Kaggle-OnlyデータとPersuadeデータの特性を考慮した2段階学習。
- **アーキテクチャ:** DeBERTa-v3-large。カスタム分類ヘッド。
- **アルゴリズム:** BCEWithLogitsLoss（順序回帰）。
- **テクニック:**
    - **データ:** Kaggleデータ、nbroadの生成データセット（低重み）。
    - **前処理:** サブストリング（単語、句読点、空白）に基づくトークナイズ、B-/I-プレフィックスの除去、空白の無視。
    - **学習:** 2段階学習（Kaggle-Persuadeで事前学習後、Kaggle-Onlyでファインチューニング）、MLM事前学習（10エポック）。
    - **後処理:** `NAME_STUDENT` のフィルタリング（title-cased、長さ1より大きい）、同一文書内でのラベル伝播、`\n` を `STREET_ADDRESS` として予測。
    - **アンサンブル:** 6つのDeBERTa-v3-largeモデルの投票。

**3位**

- **アプローチ:** MLM事前学習済みDeBERTaと、データセットの特性に基づいた2段階ファインチューニング。ソフトラベリング、敵対的サンプリング（データセット改善）。
- **アーキテクチャ:** DeBERTa-v3-base、DeBERTa-v3-large。
- **アルゴリズム:** BCE損失。
- **テクニック:**
    - **データ:** mpwareデータセットで事前学習、競技データとnbroadデータセットでファインチューニング。
    - **データ前処理:** 改行文字を特殊トークンに変換。
    - **CV戦略:** Prompt name/scoreに基づいたMultilabelstratifiedkfold。Data AとData Bを別々に分割。
    - **モデルと学習:** MLM事前学習、レイヤー凍結（9層または6層）。回帰とBCE損失を使用。2段階学習でソフトラベリングを活用。
    - **ポストプロセッシング:** `O` ラベルの閾値処理、OOF予測に基づいた偽陽性フィルタリング（短すぎる予測、教師名、Mr/Mrs/Drなど）、同一文書内でのラベル伝播、`PHONE_NUM` を `ID_NUM` に変換、URL/EMAILの除去、正規表現。

**4位**

- **アプローチ:** Llama3 🦙 70Bをベースモデルとしたアンサンブル。データソースのタグ付け、データソース分類ヘッドの追加、非Persuadeデータのスコアに基づく早期停止。動的マイクロバッチ照合、高速なDeBERTa実装。
- **アーキテクチャ:** DeBERTa V3 Large、Qwen2-1.5B-Instruct、Llama3 70B。
- **アルゴリズム:** Focal Loss。
- **テクニック:**
    - **データ:** 公式データ、33kデータ、nbroadデータセット。LLaMA3 70Bで生成したデータ。
    - **データソースの区別:** 入力にデータソースのタグを追加。データソース分類ヘッドを追加。
    - **早期停止:** 非Persuadeデータのスコアに基づいて早期停止。
    - **データ生成:** ペルソナ、ツール、PII情報を用いたLLMによるデータ生成。偽陽性サンプルの言い換え。
    - **後処理:** STUDENT_NAMEのフィルタリング（教師名、フィクションキャラクター）、正規表現による名前/IDのフィルタリング、特定のinstructor名の除去、XGBoostによる検証。
    - **その他:** Dynamic Micro Batch Collation、高速なDeBERTa実装。

**5位**

- **アプローチ:** 170万件のトレーニング例とドメイン適応。教師モデル（DeBERTa、Mamba）でテストデータをラベル付けし、その予測を模倣するように生徒モデル（DeBERTa）を訓練する。
- **アーキテクチャ:** DeBERTa-v3-large、Mamba-790m (教師モデル)、DeBERTa-v3-large (生徒モデル)。
- **アルゴリズム:** SCS損失（教師モデル）、MSE/MAE損失（生徒モデル）。カットミックス。
- **テクニック:**
    - **データ:** PERSUADE essays、Uncopyrighted Pile Completions、SlimPajama Completions、Tricky Crawl。疑似ラベリング（ソフトラベル）。
    - **データ拡張:** バグのあるスペルチェック、ブラックリスト文字の削除、タイポの追加、ランダムな大文字化、文の入れ替え。カットミックス（p=1.0）。
    - **ドメイン適応:** 短いコンテキストの生徒モデルによる教師アンサンブルの予測の模倣。
    - **アンサンブル:** 異なるバックボーンのアンサンブル、TTAライクなアンサンブル。

**6位**

- **アプローチ:** 重み付き平均アンサンブル（DeBERTaモデル）。DeBERTaトークナイザからSpacyトークナイザへのマッピング。Mistral-v02で作成した追加データセットの利用。エラー分析に基づいた後処理。
- **アーキテクチャ:** DeBERTa-v3-large、DeBERTa-v3-base、カスタムヘッド（トークンマッピング機能付き）。
- **アルゴリズム:** Focal Loss。
- **テクニック:**
    - **カスタムヘッド:** DeBERTaトークンからSpacyトークンへの予測確率のマッピング。
    - **データ:** 競技データ、公開データセット、Mistral-7B-Instruct-v0.2で生成したデータセット。
    - **損失:** DeBERTaトークン損失、文字損失、Spacyトークン損失、開始/終了位置損失。
    - **後処理:** 空白文字のラベル修正、`\n` の処理、同一エッセイ内の `NAME_STUDENT` の統一。

**7位**

- **アプローチ:** 14モデルのアンサンブルとポストプロセッシング。公開されている事前学習済みモデルの利用。
- **アーキテクチャ:** DeBERTa-v3-large（様々な事前学習済みモデル）。
- **アルゴリズム:** 不明（write-upに詳細な記述なし）。
- **テクニック:**
    - **アンサンブル:** 複数の公開モデルの重み付き投票。短いテキストにはより多くのモデルを使用。
    - **ポストプロセッシング:** ラベルごとの異なる閾値、`NAME_STUDENT` のフィルタリング（title-cased、数字/アンダースコアなし）、`B-` トークンの修正（`I-` が続かない場合）、アドレスのラベル追加（改行など考慮）。

**9位**

- **アプローチ:** DeBERTa v3 largeのアンサンブル（レイヤードロップアウト、マルチサンプルドロップアウト、LoRA）。Mixtral 8x7bで作成した改善版データセットの利用。手動で修正したtrain.json。
- **アーキテクチャ:** DeBERTa v3 large、DeBERTa v2 xlarge (LoRA)。
- **テクニック:**
    - **データ:** Mixtral 8x7bで作成したデータセット（引用、チームメイト名、URLなど追加）、手動で修正したtrain.json。
    - **学習:** 4フォールドで設定を見つけ、全データで学習。エポックごとに名前を入れ替えるコールバック。
    - **ポストプロセッシング:** `NAME_STUDENT` のフィルタリング（title-cased、英字とドットのみ）、dr/mr/missなどの除去、同一文書内の名前の統一、`I-` の修正、URL/EMAILの除去、電話番号の正規表現。