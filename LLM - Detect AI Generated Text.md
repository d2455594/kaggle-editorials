---
tags:
  - Kaggle
url: https://www.kaggle.com/competitions/llm-detect-ai-generated-text
startdate: 2023-11-01
enddate: 2024-01-23
---
**全体的な傾向:**

このコンペでは、エッセイが人間によって書かれたものか、AIによって生成されたものかを検出することが課題です。上位解法では、大規模言語モデル（LLM、特にDeBERTa-v3 largeやMistral 7b）のファインチューニングと、伝統的な機械学習手法（TF-IDF、SVM、ランダムフォレスト、GBDT）の組み合わせが一般的です。敵対的生成、データ拡張、疑似ラベリング、アンサンブル学習、そして特に、多様で大規模な学習データの作成が重要なテクニックとして用いられました。

**各解法の詳細:**

**1位**

- **アプローチ:** 多様で大規模なAI生成エッセイデータセットの作成に重点を置くことで、汎化性能の高いモデルを目指す。モデル自体よりもデータセットの質が重要であると考える。
- **アーキテクチャ:** Mistral 7b (Q)LoRAファインチューニング、DeBERTa-v3 (分類、カスタムトークナイザ+MLM+疑似ラベル、ランキング)、Ghostbuster (Llama 7b, Tiny Llama 1.1B)、Ahmetの教師なしアプローチのバリエーション。
- **アルゴリズム:** (Q)LoRA、BCE損失、ペアワイズ損失、SVM、ランダムフォレスト。
- **テクニック:**
    - **データセット:** Persuadeコーパス全体、OpenAI GPT2 output dataset、ELLIPSE corpus、NarrativeQA、wikipedia、NLTK Brown corpus、IMDB映画レビューなどの多様な人間テキストと、様々なLLM（独自、オープンソース、ファインチューニング済み）で生成したテキストを組み合わせた大規模データセット。データ拡張（スペルチェック、文字の削除/挿入/置換、同義語置換、難読化、バックトランスレーション、ランダムな大文字化、文の入れ替え）も実施。
    - **モデリング:** 複数のLLMのファインチューニング、DeBERTa-v3の様々なタスクへの適用、Ghostbuster（トークン確率ベース）、教師なしアプローチの修正実装。
    - **アンサンブル:** ランク平均を使用。Mistral-7bモデルに高い重みを設定。

**2位**

- **アプローチ:** 大規模で多様なAI生成テキストデータセットでの事前学習（pretraining）を重視し、その後、タスク固有のデータでファインチューニング（finetuning）を行う。
- **アーキテクチャ:** DeBERTa-v3-large、DeBERTa-large。
- **アルゴリズム:** 不明（write-upに詳細な記述なし）。
- **テクニック:**
    - **事前学習:** SlimPajamaデータセットからLLMで生成した約50万件の人間/AIテキストペアでDeBERTaモデルを事前学習。
    - **ファインチューニング:** Persuadeコーパスの学生エッセイで言語モデル（LM）ファインチューニング（llm-studioを使用）。DAIGT-V4-TRAIN-DATASETでもファインチューニング。
    - **アンサンブル:** 複数のDeBERTaモデルのアンサンブル。TF-IDFカーネルとのブレンドも試行。

**3位**

- **アプローチ:** TF-IDFパイプラインとDeBERTa-v3-largeモデルのアンサンブル。大規模なAI生成テキストデータセットでの事前学習と、敵対的サンプリングによるデータセットの改善。
- **アーキテクチャ:** DeBERTa-v3-base、DeBERTa-v3-large、CatBoost、LightGBM、Lightautoml、Shallow NN。
- **アルゴリズム:** TF-IDF、CatBoost、LightGBM、Lightautoml、Ridge回帰（アンサンブル）、UMAP（後処理）。
- **テクニック:**
    - **データ前処理:** 難読化解除（一定以上のエラーがあるテキストのみ）、不要な記号の削除、エンコーディングの正規化。
    - **LLM生成データ:** 厳選された約11kの生成/言い換え/部分的に言い換えられたエッセイでモデルを訓練。敵対的サンプリング（誤予測サンプルをデータセットに追加）。PileとSlimPajamaデータセットから約100万件のテキストを生成。
    - **TF-IDFパイプライン:** 既存の公開ノートブックを調整（イテレーション数の増加、疑似ラベルの追加）。
    - **アンサンブル:** TF-IDFとLLMモデルの予測を2段階で重み付け平均。
    - **後処理:** プロンプトIDごとにUMAPを用いて距離を計算し、予測を調整（クリッピング）。

**4位**

- **アプローチ:** 複数のモデル（古典的ML、LLMベース）を組み合わせたアンサンブル。データソース（Persuadeと非Persuade）の区別を考慮した学習戦略。
- **アーキテクチャ:** 線形モデル、勾配ブースティングモデル、Mistral-7bベースの特徴量抽出器、Longformer、DeBERTa-v3。
- **アルゴリズム:** TF-IDF、線形モデル、勾配ブースティング、Mistral-7b（特徴量抽出）、Longformer、DeBERTa-v3。
- **テクニック:**
    - **古典的ML:** より広いngram範囲、特徴量空間の制限、追加のトークナイザー前処理ステップ、DAIGT V2トレーニングデータ（後処理あり）、人工的なタイポの導入（ランダム）。
    - **LLMアプローチ:** Mistral-7bベースの特徴量、TF-IDFアプローチで疑似ラベル化されたテストセットでの再訓練。
    - **Transformer:** Longformer（ターゲットをデータソースとして訓練）、DeBERTa-v3（大規模な公開データで事前学習）。
    - **アンサンブル:** 各モデルに個別の重みを設定（公開LBスコアに過度に依存せず、バランスを重視）。動的マイクロバッチ照合、高速なDeBERTa実装。

**6位**

- **アプローチ:** 事前学習済みLLM（phi-2）を用いてエントロピーベースの合成特徴量を計算し、人間が書いたエッセイのみでOne-Class SVMを訓練する教師なしアプローチ。
- **アーキテクチャ:** phi-2 (LLM)、One-Class SVM。
- **アルゴリズム:** One-Class SVM。
- **テクニック:**
    - **特徴量抽出:** 事前学習済みLLM（phi-2）を用いてエントロピーベースの合成特徴量を計算。
    - **モデル訓練:** 人間が書いたエッセイのみを訓練データとしてOne-Class SVMを訓練。
    - **データ:** DAIGT-V4-TRAIN-DATASETを使用して最適な特徴量を選択。

**7位**

- **アプローチ:** 指示チューニングされていないモデル（Falcon-7B、Mistral-7B、Llama2-7B）のみを使用してAI生成データを生成し、そのデータでDeBERTa-v3-largeをファインチューニング。
- **アーキテクチャ:** DeBERTa-v3-large。
- **アルゴリズム:** 不明（write-upに詳細な記述なし）。
- **テクニック:**
    - **AI生成データ:** 指示チューニングされていないLLMを用いて、温度、top p、頻度ペナルティを調整しながら非エッセイテキストとエッセイを生成。Persuade 2.0のエッセイをプロンプトとして利用し、生成されたテキストをフィルタリング。
    - **ファインチューニング:** 生成されたデータセットでDeBERTa-v3-largeをファインチューニング。
    - **後処理:** 予測値が特定のパーセンタイル範囲内にある場合に、基本的なTF-IDF + SGDモデルの出力で置換。

**8位**

- **アプローチ:** 言語モデルのperplexity（PPL）とGLTR（Giant Language Model Test Room）の言語学的特徴量を組み合わせて、VotingClassifierを訓練する。
- **アーキテクチャ:** GPT-2 (small, medium, large)、VotingClassifier。
- **アルゴリズム:** Perplexity、GLTR (Test-2特徴量)、VotingClassifier。
- **テクニック:**
    - **特徴量抽出:** GPT-2モデルを用いてテキストレベルと文レベルのPPLを計算。GLTRを用いてTop-10、Top-100、Top-1000、1000+ランクのトークン数を計算。
    - **データ:** daigt v2データセットとstarblasters8氏が共有した800kデータセットを使用。
    - **モデル訓練:** 上記の特徴量を組み合わせてVotingClassifierを訓練。

**9位**

- **アプローチ:** TF-IDFベースのパイプラインとBERTベースのパイプラインのアンサンブル。多様なデータセットと難読化解除パイプラインを特徴とする。
- **アーキテクチャ:** TF-IDF、DeBERTa-v3-large、RoBERTa。
- **アルゴリズム:** TF-IDF、DeBERTa-v3-large、RoBERTa。
- **テクニック:**
    - **データセット:** 多様なプロンプトとサイズの異なるモデル（~20万から~70万サンプル）で慎重にキュレーションされたデータセット。
    - **難読化解除:** 統計ベースとリバースエンジニアリングベースの難読化解除パイプライン。
    - **後処理:** クラスタリングベースの後処理。
    - **BERTベースパイプライン:** 2つのDeBERTa-v3-largeモデルと1つのRoBERTaモデルを、異なるが重複する多様なデータセットのバリアントで訓練。